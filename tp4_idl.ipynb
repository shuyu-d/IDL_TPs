{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/shuyu-d/IDL_TP/blob/master/tp4_idl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 4 (Exercice 4.5) - Modèle de régression logistique\n",
    "\n",
    "Dans ce TP, nous implémentons les fonctions pour le calcul du risque empirique du modèle de régression logistique ainsi que son gradient. \n",
    "\n",
    "Les données d'observation appartiennent au jeu de données Iris, déjà disponible via scikit-learn (`sklearn datasets`, voir \"Préparation du jeu de données\"). Plus d'information sur ce jeu de données : https://scikit-learn.org/1.4/auto_examples/datasets/plot_iris_dataset.html,   https://archive.ics.uci.edu/dataset/53/iris. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation du jeu de données \n",
    "Ce jeu de données contient trois espèces différentes d’iris (Setosa, Versicolour et Virginica), caractérisées par la longueur et la largeur des sépales et des pétales. \n",
    "Les données sont stockées dans un tableau NumPy de dimension 150 x 4.\n",
    "\n",
    "Les lignes correspondent aux observations (échantillons) et les colonnes représentent respectivement : longueur du sépale, largeur du sépale, longueur du pétale et largeur du pétale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['label'] = [iris.target_names[y] for y in iris.target]\n",
    "iris_df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification binaire (Setosa versus non-Setosa) \n",
    "Avec le jeu de données Iris, on considère le problème de classification binaire qui vise à distinguer l'espèce Setosa des deux autres$^*$. Ainsi, la classe binaire $y_i$ de chaque échantillon est définie par \n",
    " * $y_i = 0$ si le label de l'échantillon est 'setosa'. On appelle cette classe 'setosa'\n",
    " * $y_1 = 1$ sinon. On appelle cette classe 'non-setosa'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iris_data = iris.data.copy() \n",
    "bin_target = iris.target.copy()\n",
    "bin_target_names = iris.target_names.copy()\n",
    "bin_target[ bin_target != 0] = 1\n",
    "bin_target_names[bin_target_names != bin_target_names[0]] = 'non-setosa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 0], iris_data[:, 1], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 0], iris_data[:, 2], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[2])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 0], iris_data[:, 3], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[3])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "## \n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 1], iris_data[:, 2], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[1], ylabel=iris.feature_names[2])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 1], iris_data[:, 3], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[1], ylabel=iris.feature_names[3])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 2], iris_data[:, 3], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[2], ylabel=iris.feature_names[3])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification binaire (Setosa vs non-Setosa) avec les données de \"Petal length\"\n",
    "On définit $X$ la variable associée au longueur du pétale ('petal length'). Maintenant l'ensemble $\\mathcal{D} = (x_i, y_i)_{i=1}^n$ est bien défini. La figure suivante est une visualisation de cet ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata = np.array( iris_data[:, 2] ) # features is 'petal length (cm)'\n",
    "y = bin_target # 0 means class 'setosa', 1 means class 'non-setosa' (the other two types)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,5))\n",
    "ax.scatter(Xdata, y, marker='o', color='b')\n",
    "ax.set_xlabel('x (petal length (cm))'); ax.set_ylabel('y (class)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle logistique recherché est de la forme $ \\hat{h}(x) = \\sigma(w^T x + b)$, òu $\\sigma(\\cdot)$ est la fonction sigmoïd. \n",
    "De manière similaire à l'écriture compacte pour la régression linéaire, on définit le paramètre $\\beta$ par $\\beta :=(b, w)^{T}\\in\\mathbb{R}^2$. Par conséquent, on transforme les données $x_i$ par $x_i \\leftarrow (1, x_i)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Xdata.shape[0]\n",
    "# ajuster la matrice Xdata \n",
    "X = np.column_stack((np.ones(n), Xdata)) \n",
    "print('Check out a few lines of the matrix X')\n",
    "pd.DataFrame(X).head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question (i) \n",
    "\n",
    "A l'aide de l'Exercice 4.1, on trouve qu'un point $\\beta_0 = (-3w_0, w_0)$, pour $w_0=2.5$, est un paramètre assez pertinent pour le modèle de régression logistique sur l'ensemble $\\mathcal{D}$.\n",
    "\n",
    "Justifier pourquoi ce point $\\beta_0$ donné est un paramètre pertinent. Pouvez-vous proposer un autre paramètre $\\beta$ meilleur que $\\beta_0$ (selon le risque empirique définit en cours) ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_trial = 2.5\n",
    "beta_trial = np.array([-3*w_trial, w_trial])\n",
    "print(beta_trial.shape) \n",
    "print(beta_trial)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualiser** le modèle avec $\\beta_0$ : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def h_logistic( x, beta ):\n",
    "    return sigmoid(x.dot(beta))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,5))\n",
    "ax.scatter(X[:,1], y, marker='o', color='b', alpha=0.6)\n",
    "hvals = [ h_logistic(x, beta_trial) for x in X ]\n",
    "ax.scatter(X[:,1], hvals, marker='x', color='r')\n",
    "ax.set_xlabel('x'); ax.set_ylabel('y') \n",
    "# fig.savefig('out4_linreg.pdf', format='pdf', bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question (ii) \n",
    "Compléter la fonction suivante pour le calcul du risique empirique. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empiricalRisk(X, y, beta):\n",
    "    #-----A COMPLÉTER EN REMPLAÇANT LA LIGNE SUIVANTE------ \n",
    "    J = 0.1\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualiser le risque empirique (voir Eq. (1), Exercices 4) sur le plan 2D avec les lignes de niveau : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "beta1_vals = np.linspace(beta_trial[1]*(0.1), beta_trial[1]*3, 100)\n",
    "beta0_vals = np.linspace(beta1_vals[0]*(-2.1), beta1_vals[-1]*(-3.6))\n",
    "J_vals = np.zeros(shape=(len(beta0_vals), len(beta1_vals)))\n",
    "for i in range(0, len(beta0_vals)):\n",
    "    for j in range(0, len(beta1_vals)):\n",
    "        J_vals[i,j] = compute_empiricalRisk(X, y, [[beta0_vals[i]], [beta1_vals[j]]])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].contour(beta0_vals, beta1_vals, np.transpose(J_vals))\n",
    "ax[0].plot(beta_trial[0], beta_trial[1], marker='^', color='r');\n",
    "ax[0].set_xlabel(r'$\\beta_1$'); ax[0].set_ylabel(r'$\\beta_2$')\n",
    "# fig.savefig('out4_linregContour.pdf', format='pdf', bbox_inches = 'tight')\n",
    "ax[1].contourf(beta0_vals, beta1_vals, np.transpose(J_vals))\n",
    "ax[1].plot(beta_trial[0], beta_trial[1], marker='x', color='r');\n",
    "ax[1].set_xlabel(r'$\\beta_1$'); ax[1].set_ylabel(r'$\\beta_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: On se donne une version alternative pour le calcul du risque empirique, qui est plus stable numériquement autour des points où $P(Y=1|X=x)$ est très proche de $1$ ou de $0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empiricalRisk_stable(X, y, beta):\n",
    "    n = y.shape[0]\n",
    "    z = X @ beta\n",
    "    # Note: de manière équivalente, le risque empirique peut être calculé avec le terme suivant \n",
    "    # On utilise 'np.logaddexp(0,z)' pour calculer log(1+e^z) pour une meilleure stabilité numérique\n",
    "    J = np.sum( np.logaddexp(0,z) - y * z )\n",
    "    return J\n",
    "\n",
    "beta1_vals = np.linspace(beta_trial[1]*(-20), beta_trial[1]*20, 100)\n",
    "beta0_vals = np.linspace(-50, 100, 100)\n",
    "\n",
    "J_vals = np.zeros(shape=(len(beta0_vals), len(beta1_vals)))\n",
    "for i in range(0, len(beta0_vals)):\n",
    "    for j in range(0, len(beta1_vals)):\n",
    "        J_vals[i,j] = compute_empiricalRisk_stable(X, y, [[beta0_vals[i]], [beta1_vals[j]]])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].contour(beta0_vals, beta1_vals, np.transpose(J_vals))\n",
    "ax[0].plot(beta_trial[0], beta_trial[1], marker='^', color='r');\n",
    "ax[0].set_xlabel(r'$\\beta_1$'); ax[0].set_ylabel(r'$\\beta_2$')\n",
    "# fig.savefig('out4_linregContour.pdf', format='pdf', bbox_inches = 'tight')\n",
    "ax[1].contourf(beta0_vals, beta1_vals, np.transpose(J_vals))\n",
    "ax[1].plot(beta_trial[0], beta_trial[1], marker='^', color='r');\n",
    "ax[1].set_xlabel(r'$\\beta_1$'); ax[1].set_ylabel(r'$\\beta_2$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question (iii) \n",
    "Remplir la fonction `compute_gradient(X, y, beta)`. Puis visualiser les gradients sur les quelques points donnés autour de $\\beta_0$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, beta): \n",
    "    #-----A COMPLÉTER EN REMPLAÇANT LA LIGNE SUIVANTE------\n",
    "    grad = np.array([0,0])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation de gradients $\\text{grad}f(\\beta)$ sur quelques points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = []\n",
    "bs = np.linspace(-30, 40, 7)\n",
    "for b in bs : \n",
    "    betas.append( np.array([b, -20]) )\n",
    "bs = np.linspace(-25, 50, 7)\n",
    "for b in bs : \n",
    "    betas.append( np.array([b, 40]) )\n",
    "\n",
    "ngrads = []\n",
    "for i in range(0, len(betas) ):\n",
    "        grad = compute_gradient(X, y, betas[i]  )\n",
    "        if i == 0:\n",
    "            n0 = np.linalg.norm(grad)\n",
    "        ngrad = - 20 * grad / n0\n",
    "        ngrads.append( ngrad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.array(betas)\n",
    "ngrads = np.array(ngrads)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,5))\n",
    "ax.contour(beta0_vals, beta1_vals, np.transpose(J_vals))\n",
    "ax.plot(beta_trial[0], beta_trial[1], marker='^', color='r');\n",
    "ax.set_xlabel(r'$\\beta_1$'); ax.set_ylabel(r'$\\beta_2$')\n",
    "\n",
    "ax.quiver(betas[:,0], betas[:,1], ngrads[:,0], ngrads[:,1],\n",
    "            angles='xy', scale_units='xy', scale=1, color='gray', alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question (iv) \n",
    "Donner votre observation des gradients sur les points donnés. En particulier, expliquer la tendance des normes de ces gradients par rapport à leur positions sur le plan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
