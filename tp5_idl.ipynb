{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/shuyu-d/IDL_TP/blob/master/tp5_idl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 5 (Exercice 6.3) - Perceptron multicouches (MLP)\n",
    "\n",
    "Dans ce TP, nous implémentons les fonctions pour le calcul du passe avant et du passe arrière pour entraîner un MLP construit avec Numpy.\n",
    "\n",
    "Les données d'observation appartiennent au jeu de données Iris, déjà disponible via scikit-learn (`sklearn datasets`, voir \"Préparation du jeu de données\"). Plus d'information sur ce jeu de données : https://scikit-learn.org/1.4/auto_examples/datasets/plot_iris_dataset.html,   https://archive.ics.uci.edu/dataset/53/iris. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation du jeu de données \n",
    "Ce jeu de données contient trois espèces différentes d’iris (Setosa, Versicolour et Virginica), caractérisées par la longueur et la largeur des sépales et des pétales. \n",
    "Les données sont stockées dans un tableau NumPy de dimension 150 x 4.\n",
    "\n",
    "Les lignes correspondent aux observations (échantillons) et les colonnes représentent respectivement : longueur du sépale, largeur du sépale, longueur du pétale et largeur du pétale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['label'] = [iris.target_names[y] for y in iris.target]\n",
    "iris_df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification binaire (Setosa versus non-Setosa) \n",
    "Avec le jeu de données Iris, on considère le problème de classification binaire qui vise à distinguer l'espèce Setosa des deux autres$^*$. Ainsi, la classe binaire $y_i$ de chaque échantillon est définie par \n",
    " * $y_i = 0$ si le label de l'échantillon est 'setosa'. On appelle cette classe 'setosa'\n",
    " * $y_1 = 1$ sinon. On appelle cette classe 'non-setosa'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iris_data = iris.data.copy() \n",
    "bin_target = iris.target.copy()\n",
    "bin_target_names = iris.target_names.copy()\n",
    "bin_target[ bin_target != 0] = 1\n",
    "bin_target_names[bin_target_names != bin_target_names[0]] = 'non-setosa' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 0], iris_data[:, 1], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 0], iris_data[:, 2], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[2])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 0], iris_data[:, 3], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[3])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "## \n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 1], iris_data[:, 2], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[1], ylabel=iris.feature_names[2])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 1], iris_data[:, 3], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[1], ylabel=iris.feature_names[3])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(iris_data[:, 2], iris_data[:, 3], c=bin_target)\n",
    "ax.set(xlabel=iris.feature_names[2], ylabel=iris.feature_names[3])\n",
    "_ = ax.legend(scatter.legend_elements()[0], bin_target_names, loc=\"lower right\", \n",
    "    bbox_to_anchor=(0.15,1.02,1,0.2),ncol=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit $X$ le vecteur aléatoire associé aux quatres charactéristiques dans le jeu de données Iris. Maintenant l'ensemble $\\mathcal{D} = (x_i, y_i)_{i=1}^n$ avec $x_i\\in\\mathbb{R}^4$ et $y_i \\in \\{0,1\\}$. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array( iris_data ) \n",
    "y = bin_target # 0 means class 'setosa', 1 means class 'non-setosa' (the other two types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On divise le jeu de données en deux parties : une partie pour l'entraînement du MLP, et une partie complémentaire pour évaluer la prédiction par le MLP entraîné. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check out a few lines of the matrix X (shape %d x %d)'%(X.shape) )\n",
    "print( pd.DataFrame(X).head() )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "n, p = X_train.shape\n",
    "\n",
    "print('A few rows of Xtrain (shape %d x %d) : \\n' %(n,p), pd.DataFrame(X_train).head() ) \n",
    "print('A few rows of y_train (shape %d x 1): \\n'%y_train.shape[0], pd.DataFrame(y_train).head() ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Construction et initialisation d'un MLP \n",
    "\n",
    "**Parametres du MLP** \n",
    "\n",
    "* Entrées : $p=4$ \n",
    "* 2 couches cachées : $\\sigma_1=$ReLU, $\\sigma_2=$ReLU\n",
    "    * couche cachée 1: 3 neurones\n",
    "    * couche cachée 2: 4 neurones\n",
    "* Sortie : \n",
    "    * $\\sigma_3$=sigmoid \n",
    "    * $y=f_{\\theta}(x) \\in [0,1]$ \n",
    "  \n",
    "**Risque empirique** \n",
    "\n",
    "On définit le risque empirique suivant comme l'objectif de l'entraînement : \n",
    "$$ R(\\theta) = \\frac{1}{2}\\sum_{i=1}^n ( f_{\\theta}(x_i) - y_i )^2.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres du MLP (2 couches cachées : sigma_1=ReLU, sigma_2=ReLU, sortie sigma_3=sigmoid)\n",
    "#  - couche cachée 1: 3 neurones\n",
    "#  - couche cachée 2: 4 neurones\n",
    "np.random.seed(0)\n",
    "\n",
    "W1 = np.random.randn(p, 3) * 0.1\n",
    "b1 = np.zeros((1, 3))\n",
    "\n",
    "W2 = np.random.randn(3, 4) * 0.1\n",
    "b2 = np.zeros((1, 4))\n",
    "\n",
    "W3 = np.random.randn(4, 1) * 0.1\n",
    "b3 = np.zeros((1, 1))\n",
    "\n",
    "# Fonctions d'activation\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Dérivée de la fonction ReLU \n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question (a) : passe avant\n",
    "\n",
    "A l'aide de l'Exercice 6.2 (i), compléter la fonction `passe_avant()` pour l'évaluation de la fonction de prédiction $f_{\\theta}(\\cdot)$ sur un ensemble de données $X$, étant donné le paramètre $\\theta=(W^{(i)}, b^{(i)})_{i=1,2,3}$ du MLP. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passe avant \n",
    "def passe_avant(X, W1, W2, W3, b1, b2, b3): \n",
    "    # ---- A COMPLÉTER ------\n",
    "    Z1 = \n",
    "    A1 = \n",
    "    Z2 = \n",
    "    A2 = \n",
    "    Z3 =  \n",
    "    Y  = \n",
    "    # -----------------------\n",
    "    return (Z1, A1, Z2, A2, Z3, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question (b) : passe arrière \n",
    "A l'aide des équations pour le calcul des dérivées partielles $\\frac{\\partial R}{\\partial W^{(\\ell)}_{ij}}$, compléter les lignes pour le passe arrière dans le bloc de l'algorithme GD suivant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement par rétropropagation\n",
    "\n",
    "# Algorithme : décente de gradient avec stepsize fixe \n",
    "#   lr (pour learning rate): un stepsize fixe   \n",
    "#   niters_max             : nombre maximal d'iterations \n",
    "lr = 0.05\n",
    "niters_max = 5000\n",
    "\n",
    "for niter in range(niters_max):\n",
    "    # Passe avant \n",
    "    (Z1, A1, Z2, A2, Z3, Y_hat) = passe_avant(X_train, W1, W2, W3, b1, b2, b3)\n",
    "    \n",
    "    # Observer la fonction de perte par la cross-entropie \n",
    "    loss = - np.mean(y_train * np.log(Y_hat + 1e-8) + (1 - y_train) * np.log(1 - Y_hat + 1e-8) )\n",
    "\n",
    "    # Retropropagation\n",
    "    \n",
    "    #-------A COMPLÉTER -----------\n",
    "    dZ3 =  # delta^3 := dR/dz^{(3)} de l'Eq (1)\n",
    "    dW3 =  # dR/dW^{(3)}\n",
    "    db3 =  # dR/db^{(3)}\n",
    "\n",
    "    dA2 =  # nom suggéré pour une dérivée partielle intermédiare \n",
    "    dZ2 =  # d(delta^3)/dz^{(2)} de l'Eq (2)\n",
    "    dW2 = \n",
    "    db2 = \n",
    "\n",
    "    dA1 =  # nom suggéré pour une dérivée partielle intermédiare \n",
    "    dZ1 = \n",
    "    dW1 = \n",
    "    db1 = \n",
    "    #--------------------------------\n",
    "\n",
    "    # Décente de gradient \n",
    "    W3 -= lr * dW3\n",
    "    b3 -= lr * db3\n",
    "\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "\n",
    "    if niter % 1000 == 0:\n",
    "        print(f\"niter {niter}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du MLP \n",
    "outputs = passe_avant(X_test, W1, W2, W3, b1, b2, b3)\n",
    "Y_hat_test = outputs[-1]\n",
    "\n",
    "predictions = (Y_hat_test > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
