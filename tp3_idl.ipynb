{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/shuyu-d/IDL_TPs/blob/master/tp3_idl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 (Exercice 3.5) - Calculs des gradients \n",
    "\n",
    "Dans ce TP, nous implémentons les fonctions pour le calcul des gradients de la fonction objective du problème de regression linéaire. \n",
    "\n",
    "Les données d'observation est généré aléatoirement par une fonction dédiée de Scikit-Learn (`sklearn`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparer un jeu de données\n",
    "\n",
    "Dans cet example, on génère un jeu de données de manière aléatoire par `sklearn.datasets.make_regression`. Le nombre de variables dans $\\bar{x}$ est $p=1$, et le nombre d'échantillons observés est $n=100$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Xdata, y = datasets.make_regression(n_samples=30, n_features=1, noise=30) # \n",
    "Xdata, y = datasets.make_regression(n_samples=100, n_features=1, noise=30) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observer une paritie du jeu de données avec l'aide de Pandas DataFrame. Les `DataFrame`s de Pandas sont souvent plus facile à visualiser que les structures de données de NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata_pd = pd.DataFrame( Xdata )\n",
    "Xdata_pd.head() # voir quelques lignes de la matrix Xdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualiser les données d'observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,5))\n",
    "ax.scatter(Xdata, y, marker='o', color='b')\n",
    "ax.set_xlabel('x'); ax.set_ylabel('y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dans le contexte de l'Exercice 2.7, calculer la solution du problème de régression linéaire \n",
    "Le modèle linéaire recherché est de la forme $ \\bar{y} = {\\beta^{\\star}}^{T} \\bar{x} + \\epsilon$. Ici, dans le cas où $\\bar{x}$ contient une seule variable prédictive ($p=1$), le produit scaliare entre $\\beta^{\\star}$ et $\\bar{x}$ se réduit au simple produit $\\beta^{\\star} \\bar{x}$ où le regresseur $\\beta^{\\star} \\in \\mathbb{R}$ représente la pente recherchée du modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Xdata.shape[0]\n",
    "# ajuster la matrice Xdata \n",
    "print(r'A few rows of the matrix X for linear regression, with the column $(1,..,1)^T$ appended to the design matrix of the dataset:')\n",
    "X = np.hstack((np.ones((n,1)), Xdata)) \n",
    "pd.DataFrame(X).head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_closed_form(X, y):\n",
    "    # implémenter l'expression explicite de la solution recherchée \n",
    "    # inv = np.linalg.pinv(X.T.dot(X)) # utile \n",
    "    inv = np.linalg.inv(X.T.dot(X))\n",
    "    beta = inv.dot(X.T).dot(y)\n",
    "    return beta\n",
    "\n",
    "beta_closedform = beta_closed_form(X, y)\n",
    "print(beta_closedform.shape) # expected shape: (2,)\n",
    "print(beta_closedform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualiser** la solution : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,5))\n",
    "ax.scatter(X[:,1], y, marker='o', color='b')\n",
    "ax.plot(X[:,1], X.dot(beta_closedform), color='r')\n",
    "ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(4,7))\n",
    "# ax.scatter(X[:,1], y, marker='o', color='k')\n",
    "# ax.plot(X[:,1], X.dot(beta_closedform), color='b')\n",
    "# ax.grid()\n",
    "# ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "fig.savefig('out4_linreg.pdf', format='pdf', bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualiser la solution sur le plan montrant les lignes de niveau de la fonction d'objective : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_cost_one_variable(X, y, beta):\n",
    "    n = y.shape[0]\n",
    "    J = (1/(2*n)) * (np.sum((X.dot(beta) - y)**2))\n",
    "    return J\n",
    "beta0_vals = np.linspace(beta_closedform[0]*(-60), beta_closedform[0]*60, 100)\n",
    "beta1_vals = np.linspace(beta_closedform[1]*(-8), beta_closedform[1]*8, 100)\n",
    "J_vals = np.zeros(shape=(len(beta0_vals), len(beta1_vals)))\n",
    "for i in range(0, len(beta0_vals)):\n",
    "    for j in range(0, len(beta1_vals)):\n",
    "        J_vals[i,j] = compute_cost_one_variable(X, y, [[beta0_vals[i]], [beta1_vals[j]]])\n",
    "\n",
    "# print(J_vals)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,5))\n",
    "ax.contour(beta0_vals, beta1_vals, np.transpose(J_vals))\n",
    "ax.plot(beta_closedform[0], beta_closedform[1], marker='x', color='r');\n",
    "ax.set_xlabel(r'$\\beta_1$'); ax.set_ylabel(r'$\\beta_2$')\n",
    "fig.savefig('out4_linregContour.pdf', format='pdf', bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.5 \n",
    "Le modèle linéaire recherché est de la forme $ y = {\\beta^{\\star}}^{T} x + \\epsilon$. Ici, dans le cas où $x$ contient une seule variable prédictive ($p=1$), le produit scaliare entre $\\beta^{\\star}$ et $x$ se réduit au simple produit $\\beta^{\\star} x$ où le regresseur $\\beta^{\\star} \\in \\mathbb{R}$ représente la pente recherchée du modèle.\n",
    "\n",
    "**Question (i):** A l'aide du résultat de l'Exercice 3.2, montrer que la fonction objective $f$ de ce problème est dérivable. En même temps, donner l'expression du gradient de $f$ en $\\beta\\in\\mathbb{R}^2$.\n",
    "\n",
    "Remplir la fonction `compute_gradient(X, y, beta)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, beta):\n",
    "    # ----A COMPLÉTER-------\n",
    "    # Remplacer la ligne suivante pour calculer le gradient \n",
    "    grad = np.array([0,0])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation de gradients $\\text{grad}f(\\beta)$ sur quelques points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On se donne quelques points autour de la solution \\beta^* sur le plan \n",
    "betas = []\n",
    "beta_ = np.array([-beta_closedform[0]*25, beta_closedform[1]*8])\n",
    "betas.append(beta_)\n",
    "\n",
    "beta_ = np.array([beta_closedform[0]*25, beta_closedform[1]*8])\n",
    "betas.append(beta_)\n",
    "\n",
    "beta_ = np.array([-beta_closedform[0]*20, beta_closedform[1]*4])\n",
    "betas.append(beta_)\n",
    "\n",
    "beta_ = np.array([beta_closedform[0]*20, beta_closedform[1]*4])\n",
    "betas.append(beta_)\n",
    "\n",
    "beta_ = np.array([-beta_closedform[0]*25, -beta_closedform[1]*8])\n",
    "betas.append(beta_)\n",
    "\n",
    "beta_ = np.array([beta_closedform[0]*25, -beta_closedform[1]*8])\n",
    "betas.append(beta_)\n",
    "\n",
    "beta_ = np.array([-beta_closedform[0]*20, -beta_closedform[1]*4])\n",
    "betas.append(beta_)\n",
    "\n",
    "beta_ = np.array([beta_closedform[0]*20, -beta_closedform[1]*4])\n",
    "betas.append(beta_)\n",
    "\n",
    "ngrads = []\n",
    "for i in range(0, len(betas) ):\n",
    "        grad = compute_gradient(X, y, betas[i]  )\n",
    "        if i == 0:\n",
    "            n0 = np.linalg.norm(grad)\n",
    "        # Note 1: Renormaliser les gradients par une scaliare CONSTANTE ajustée \n",
    "        # afin de bien visualiser ces gradients dans le cadre de la figure.   \n",
    "        # Note 2 : Raison pour le signe '-', c'est qu'on s'intéresse en effect \n",
    "        # un vecteur dans la direction opposée de chaque gradient.\n",
    "        ngrad = -100 * grad / n0\n",
    "        ngrads.append( ngrad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.array(betas)\n",
    "ngrads = np.array(ngrads)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,5))\n",
    "ax.contour(beta0_vals, beta1_vals, np.transpose(J_vals))\n",
    "ax.plot(beta_closedform[0], beta_closedform[1], marker='x', color='r');\n",
    "ax.set_xlabel(r'$\\beta_1$'); ax.set_ylabel(r'$\\beta_2$')\n",
    "\n",
    "# visualiser les vecteurs opposés des gradients calculés \n",
    "ax.quiver(betas[:,0], betas[:,1], ngrads[:,0], ngrads[:,1],\n",
    "            angles='xy', scale_units='xy', scale=1, color='k', alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question (ii) :** Donner votre observation de la visualisation des gradients sur les points donnés. En particulier, expliquer la tendance des normes de ces gradients par rapport à leur positions sur le plan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
